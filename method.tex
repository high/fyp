%!TEX root = /Users/high/Documents/School/Thesis/report/thesis.tex

\section{Method} % (fold)
\label{sec:method}

In this chapter, methods for achieving all the objectives are discussed in more detail.

\subsection{Define implementation scope for PRiDe} 
\label{subsec:impl_bound}

A literature analysis of alternative protocol specifications is conducted to achieve domain knowledge and protocol attributes about PRiDe. The protocol specification will include features that are used by PRiDe, both required and optional features to guarantee replication correctness. 

\subsection{Implement PRiDe on a  platform} 
\label{subsec:create_implementation}

The DeeDS architecture and the PRiDe protocol have a number of requirements that are required in a implementation. 
First, an analysis is performed to determine these requirements. Secondly, a list of distributed database candidates that fulfill these requirements as good as possible is produced. Finally, a decision is made on which platform to used based on the requirements.

Two alternatives have been identified for an implementation that follows the DeeDS architecture.

\begin{description}
 
\item[Create a layer on top of Berkeley DB] \
	
A implementation of PRiDe will be implemented on top of Berkeley DB, using Berkeley DB as local database with replication disabled in Berkeley DB. This type of implementation has also been performed in Dynamo, as described by \cite{Dynamo2007}. In Dynamo, BerkleyDB acts as local database manager with no replication support and the replication is done by a upper layer. Dynamo supports optimistic replication so that writes can always be performed(e.g. not blocked by other writes) and it supports eventual consistency so that any inconsistencies created by the replication can be resolved later. This means that Dynamo has similar features as PRiDe and motivates why PRiDe can be implemented in the same way.  
	
\item[Integrate in Berkeley DB] \

The implementation will be inserted inside Berkeley DB to change the replication semantics of Berkeley DB and to reuse some of the features that Berkeley DB has. A technical report, written by \cite{kang2008}, describes that there is a number of inter-dependencies, both in the replication layer, and between the upper replication layer and lower BDB layer. These inter-dependencies are not documented at all, and will extend the development time a lot by testing and verifying the implementation since the behavior of the replication can't be derived from the documentation.
 
\end{description}
   	
PRiDe will be implemented as a layer above the replication layer in Berkeley DB. The protocol will interpret transactions coming from  Berkeley DB, and perform protocol specific instructions and then send it to the replication layer in BerkleyDB. \cite{kang2008} argues that since Berkeley DB was designed for single-master replication and there are a number of inter-depedencies which makes it hard to modify the replication layer.
%\end{description} 

\subsection{Create baseline for evaluation} % (fold)
\label{sub:extend_framework}

A baseline is a comparable implementation to PRiDe. This implementation needs to support \emph{distributed-master} replication so that independent updates are allowed. Distributed-master means that there are more than one replica writer, but each write-request are sent to the master node using a distributed transaction. 


%\subsection{Extend platform for evaluation} % (fold)
%\label{sub:extend_framework}

%\subsubsection{Requirements on the platform} % (fold)
%\label{ssub:demands_on_a_framework}

%The framework should have the same features as PRiDe has (or as close as possible). This means that the framework should have support for eventual consistency, multi-master replication, and transactions. These are features that are central to the PRiDe protocol.

%The framework should also be extendable since if there are some features that are missing, there should be easy to implement on the framework. 


%\subsubsection{Review and selection of platforms} % (fold)
%\label{ssub:rev_of_framework}

%There exists a number of frameworks that are suitable for this type of extension based on their features:

%\begin{description}
%	\item[Berkeley DB] 
%		Berkeley DB is open-source distributed database, which matches the requirement about extendability, since the source-code can be modified. Berkeley DB supports single-master replication, which is close to what we want to have, that is multi-master replication.  Berkeley DB also supports transactions which also are needed for the experiment. 
	
%	\item[Polyhedra] 
%		Polyhedra is a distributed database. It supports transactions, in-memory data storage and single-master replication.  Polyhedra is based on a client/server architecture with cross-platform interpretability. 
%	\item[Mimer SQL Real-Time Edition]
%		Mimer DB is a distributed database with support for transactions, single-master replication and real-time constraints.
%\end{description}

%Table \ref{table_features} gives a summary of support for the features that are needed by DeeDS.

%\begin{table}[h]
%	\begin{tabular}{r|c|c|c|c}
%		& \textbf{Extendable} & \textbf{Ev. Con.} & \textbf{Multi-master} & \textbf{Transactions} \\ \hline
%		BDB & X & - & - & X \\
%		MimerDB & - & - & - & X \\
%		Polyhedra & - & - & - & X \\ 
%	\end{tabular}
%	\caption[Database features]{Table showing what features the databases support that are required by DeeDS}
%	\label{table_features}
%\end{table}

%Given the features that each framework has, the chosen framework for this study is Berkeley DB. Both Polyhedra and Mimer SQL supports both transactions and single-master replication as BerkleyDB but in both cases, there is a need for a license. Berkeley DB is open-source, which means that there is no need for a license and can be modified in any way that is necessary. Since Polyhedra is based on a client/server architecture it would take longer time to extend then Berkeley DB because Berkeley DB is just a static library that you include in your source code which removes the time for developing the client for using the Polyhedra client library.  

\subsection{Evaluate the performance} 
\label{subsec:experiment}


\subsubsection{Evaluation definitions} % (fold)
\label{ssub:evaluation_baseline}

For the experiments, we define a baseline that we can evaluate with. This baseline consists of two systems. The first system is general distributed database(DDB) that uses single-master replication and uses a two-phase commit protocol. DDB will only be used as a baseline for discussing, and will not be used in the experiments. The second system is Berkeley DB but has been extended with multi-master support, will be referenced as BDBe (Berkeley DB extended). 

\subsection{Performance metrics} % (fold)
\label{sub:performance_metrics}




\subsubsection{The experiments} % (fold)
\label{ssub:the_experiments}

Three experiments have been identified that evaluates the defined performance metrics. The first two are required for this experiment, and the third experiment is optional based on the fact that is performs a validation, and not a comparison:

\begin{enumerate}

	\item \textbf{Single-master replication} \\ In this experiment, execution time and bandwidth usage will be measured and only one node will receive the transactions. For DDB, and BDBe this means that all transactions will be received on the master node. This is to measure ho much overhead PRiDe has when performing stabilization, since DDB and BDBe doesn't need to perform any stabilization since they guarantees mutual consistency. The hypothesis is that PRiDe will have none or a small overhead since every update creates a new generation that needs to be stabilized in a stabilization phase.
	
	\item \textbf{Multi-master replication} \\ In this experiment, transactions will be received on all nodes in the network for evaluating how BDBe and PRiDe handle independent updates. Execution time, blocking time and bandwidth usage will be measured. Since BDBe have some blocking time during each transaction, PRiDe should have higher throughput of updates that are locally integrated into the node that receives the update.  
	
%In this experiment, there is an interest to measure blocking time, since every node that is not a master, needs to wait until a transaction has been received, committed and propagated until next transaction can be processed. Bandwidth usage is also a interesting factor here, because this test should show that PRiDe uses less bandwidth since each transaction can be commit locally, so there is no need to send it to a master node. Execution time will also be measured since this is of interest to find out how long it takes to get all transactions performed on all nodes. A hypothesis here is that PRiDe should perform this experiment better since BerkleyDB needs an extra transfer to the master node on each transaction. 
	   
	\item \textbf{Stabilization resource usage} \\ In this experiment, the stabilization time will be measured. This will only conducted with PRiDe to evaluate if there are any spikes in resource usage when performing stabilization and conflict resolution.
	
%Last experiment the idea is mainly to measure the stabilization time. For PRiDe, this means that the time it takes to stabilize all object and resolve any conflicts. In BerkleyDB, this means measuring the time it takes to process all updates for the master and then propagate the information to the slaves. 
	 
\end{enumerate}

%Next experiment should be to see how PRiDe performs when using distributed masters in a network. Since BerkleyDB only has support for single-master, it must send each transaction to the master node so that master node can make a commit and then propagate the update to all nodes in the network and finally report back to the database node that send the transaction that it was committed or aborted.  A hypothesis here is that this should take longer to performs this task then with PRiDe because in PRiDe all database nodes are allowed to run transactions independent of each other. In the case of BerkleyDB a increase in execution time and bandwidth usage should be shown because two additional messages transactions needs to be performed.  

%A final experiment should be to test how PRiDe is performed in database network with multi-master replication. The purpose is manly to measure stabilization time but execution time and bandwidth usage should also be measured. Stabilization time can be measured in different ways. It can be measured on each individual node or measure it from start until all database nodes have propagated all updates and have integrated all updates on their local copy. Measuring on a individual node doesn't give any result that can be used since it depends on network latency when each database node receives its messages. 
